# -*- coding: utf-8 -*-
"""Detecting_Fake_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e6loOxSouMCasB1AEqMAfkW0mzWpKUGV
"""

import numpy as np
import pandas as pd
import re
import torch

!pip install transformers

!pip install xgboost==2.1.4

from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

true_df = pd.read_csv("True.csv")
fake_df = pd.read_csv("Fake.csv")

"""#Labeling

"""

true_df['label'] = 0
fake_df['label'] = 1

dataset = pd.concat([true_df, fake_df])
dataset = dataset.sample(frac=1).reset_index(drop=True)

"""#Text Preprocessing"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text

dataset['text'] = dataset['text'].apply(clean_text)

"""#Loading BERT Tokenizer and Model"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')
def get_bert_embedding(text):
    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors="pt")  # Reduce max_length for faster processing
    with torch.no_grad():
        output = bert_model(**tokens)
    return output.last_hidden_state[:, 0, :].squeeze().numpy()

"""#Compute Embeddings"""

dataset['embedding'] = dataset['text'].sample(n=3000, random_state=42).apply(get_bert_embedding)

"""#Prepare Feature Matrix"""

X = np.vstack(dataset.dropna(subset=['embedding'])['embedding'].values)  # Drop missing values if any
y = dataset.dropna(subset=['embedding'])['label'].values

"""#Train and Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Train XGBoost Classifier (Optimize hyperparameters for speed)"""

xgb_clf = XGBClassifier(n_estimators=50, learning_rate=0.2, max_depth=3)  # Reduced estimators & depth for faster training
xgb_clf.fit(X_train, y_train)

"""#Prediction"""

y_pred = xgb_clf.predict(X_test)

"""#Evaluation"""

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

import pandas as pd

# Load datasets (only once is enough â€” keep these at the top of your notebook)
real_df = pd.read_csv("True.csv")
fake_df = pd.read_csv("Fake.csv")

# Pick a random real and fake example
real_example = real_df.sample(1).iloc[0]
fake_example = fake_df.sample(1).iloc[0]

print("\nğŸŸ¢ REAL News Example:")
print("Title:", real_example['title'])
print("Text:", real_example['text'])

print("\nğŸ”´ FAKE News Example:")
print("Title:", fake_example['title'])
print("Text:", fake_example['text'])

import re
import numpy as np
import torch
from lime.lime_text import LimeTextExplainer

# ğŸ§¹ Same cleaning as training
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text

# ğŸ’¬ Take input from user
user_input = input("Enter the news text: ")
cleaned_input = clean_text(user_input)

# ğŸ§  Convert to BERT embedding (use same method from training)
embedding = get_bert_embedding(cleaned_input)
embedding = embedding.reshape(1, -1)

# ğŸ¤– Predict using loaded model
prediction = loaded_model.predict(embedding)[0]
probabilities = loaded_model.predict_proba(embedding)[0]

# ğŸ—ï¸ Map prediction to label
label_map = {0: "REAL", 1: "FAKE"}
print(f"\nğŸ§¾ The news is predicted to be: {label_map[prediction]}")
print(f"Confidence â†’ REAL: {probabilities[0]:.2f} | FAKE: {probabilities[1]:.2f}")

# âš™ï¸ Stable prediction function for LIME
def predict_fn(texts):
    embeddings = []
    for t in texts:
        # Clean each text (consistent preprocessing)
        cleaned_t = clean_text(t)
        emb = get_bert_embedding(cleaned_t)
        embeddings.append(emb.flatten())
    embeddings = np.array(embeddings)
    return loaded_model.predict_proba(embeddings)

# ğŸ§© LIME explainer (keep speed optimization)
explainer = LimeTextExplainer(class_names=['REAL', 'FAKE'])

exp = explainer.explain_instance(
    user_input,     # full user input, not cleaned one
    predict_fn,
    num_features=10,
    num_samples=500  # faster but still accurate
)

# ğŸŸ¥ğŸŸ© Display explanation
exp.show_in_notebook(text=True)

# ğŸ’¾ Optional: save explanation to HTML
exp.save_to_file('lime_explanation.html')